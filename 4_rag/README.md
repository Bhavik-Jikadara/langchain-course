# [RAG](https://python.langchain.com/v0.1/docs/use_cases/question_answering/#what-is-rag)

<div class="theme-doc-markdown markdown"><p>One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&amp;A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-rag">What is RAG?<a href="#what-is-rag" class="hash-link" aria-label="Direct link to What is RAG?" title="Direct link to What is RAG?">​</a></h3><p>RAG is a technique for augmenting LLM knowledge with additional data.</p><p>LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).</p><p>LangChain has a number of components designed to help build Q&amp;A applications, and RAG applications more generally. </p><p><strong>Note</strong>: Here we focus on Q&amp;A for unstructured data. Two RAG use cases which we cover elsewhere are:</p><ul><li><a href="/v0.1/docs/use_cases/sql/">Q&amp;A over SQL data</a></li><li><a href="/v0.1/docs/use_cases/code_understanding/">Q&amp;A over code</a> (e.g., Python)</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="rag-architecture">RAG Architecture<a href="#rag-architecture" class="hash-link" aria-label="Direct link to RAG Architecture" title="Direct link to RAG Architecture">​</a></h2><p>A typical RAG application has two main components:</p><p><strong>Indexing</strong>: a pipeline for ingesting data from a source and indexing it. <em>This usually happens offline.</em></p><p><strong>Retrieval and generation</strong>: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.</p><p>The most common full sequence from raw data to answer looks like:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="indexing">Indexing<a href="#indexing" class="hash-link" aria-label="Direct link to Indexing" title="Direct link to Indexing">​</a></h4><ol><li><strong>Load</strong>: First we need to load our data. This is done with <a href="/v0.1/docs/modules/data_connection/document_loaders/">DocumentLoaders</a>.</li><li><strong>Split</strong>: <a href="/v0.1/docs/modules/data_connection/document_transformers/">Text splitters</a> break large <code>Documents</code> into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.</li><li><strong>Store</strong>: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a <a href="/v0.1/docs/modules/data_connection/vectorstores/">VectorStore</a> and <a href="/v0.1/docs/modules/data_connection/text_embedding/">Embeddings</a> model.</li></ol><p><img loading="lazy" alt="index_diagram" src="https://python.langchain.com/v0.1/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png" width="2583" height="1299" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="retrieval-and-generation">Retrieval and generation<a href="#retrieval-and-generation" class="hash-link" aria-label="Direct link to Retrieval and generation" title="Direct link to Retrieval and generation">​</a></h4><ol start="4"><li><strong>Retrieve</strong>: Given a user input, relevant splits are retrieved from storage using a <a href="/v0.1/docs/modules/data_connection/retrievers/">Retriever</a>.</li><li><strong>Generate</strong>: A <a href="/v0.1/docs/modules/model_io/chat/">ChatModel</a> / <a href="/v0.1/docs/modules/model_io/llms/">LLM</a> produces an answer using a prompt that includes the question and the retrieved data</li></ol><p><img loading="lazy" alt="retrieval_diagram" src="https://python.langchain.com/v0.1/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png" width="2532" height="1299" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="table-of-contents">Table of contents<a href="#table-of-contents" class="hash-link" aria-label="Direct link to Table of contents" title="Direct link to Table of contents">​</a></h2><ul><li><a href="/v0.1/docs/use_cases/question_answering/quickstart/">Quickstart</a>: We recommend starting here. Many of the following guides assume you fully understand the architecture shown in the Quickstart.</li><li><a href="/v0.1/docs/use_cases/question_answering/sources/">Returning sources</a>: How to return the source documents used in a particular generation.</li><li><a href="/v0.1/docs/use_cases/question_answering/streaming/">Streaming</a>: How to stream final answers as well as intermediate steps.</li><li><a href="/v0.1/docs/use_cases/question_answering/chat_history/">Adding chat history</a>: How to add chat history to a Q&amp;A app.</li><li><a href="/v0.1/docs/use_cases/question_answering/hybrid/">Hybrid search</a>: How to do hybrid search.</li><li><a href="/v0.1/docs/use_cases/question_answering/per_user/">Per-user retrieval</a>: How to do retrieval when each user has their own private data.</li><li><a href="/v0.1/docs/use_cases/question_answering/conversational_retrieval_agents/">Using agents</a>: How to use agents for Q&amp;A.</li><li><a href="/v0.1/docs/use_cases/question_answering/local_retrieval_qa/">Using local models</a>: How to use local models for Q&amp;A.</li></ul></div>