# [Chains](https://python.langchain.com/v0.1/docs/modules/chains/)

Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.
LCEL is great for constructing your chains, but it's also nice to have chains used off the shelf. There are two types of off-the-shelf chains that LangChain supports:

* Chains that are built with LCEL. In this case, LangChain offers a higher-level constructor method. However, all that is being done under the hood is constructing a chain with LCEL.
* [Legacy] Chains constructed by subclassing from a legacy Chain class. These chains do not use LCEL under the hood but are the standalone classes.

We are working on creating methods that create LCEL versions of all chains. We are doing this for a few reasons.

1. Chains constructed in this way are nice because if you want to modify the internals of a chain you can simply modify the LCEL.
2. These chains natively support streaming, async, and batch out of the box.
3. These chains automatically get observability at each step.

This page contains two lists. First, a list of all LCEL chain constructors. Second, a list of all legacy Chains.

## 1. [LCEL Chains](https://python.langchain.com/v0.1/docs/modules/chains/#lcel-chains)

Below is a table of all `LCEL chain constructors`.

Table columns:

* **Chain Constructor**: The constructor function for this chain. These are all methods that return LCEL Runnables. We also link to the API documentation.
* **Function Calling**: Whether this requires OpenAI function calling.
* **Other Tools**: Other tools (if any) used in this chain.
* **When to Use**: Our commentary on when to use this chain.

<table><thead><tr><th>Chain Constructor</th><th>Function Calling</th><th>Other Tools</th><th>When to Use</th></tr></thead><tbody><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html#langchain.chains.combine_documents.stuff.create_stuff_documents_chain" target="_blank" rel="noopener noreferrer">create_stuff_documents_chain</a></td><td></td><td></td><td>This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.structured_output.base.create_openai_fn_runnable.html#langchain.chains.structured_output.base.create_openai_fn_runnable" target="_blank" rel="noopener noreferrer">create_openai_fn_runnable</a></td><td>✅</td><td></td><td>If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for its call, but it does not have to call it.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.structured_output.base.create_structured_output_runnable.html#langchain.chains.structured_output.base.create_structured_output_runnable" target="_blank" rel="noopener noreferrer">create_structured_output_runnable</a></td><td>✅</td><td></td><td>If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.query_constructor.base.load_query_constructor_runnable.html#langchain.chains.query_constructor.base.load_query_constructor_runnable" target="_blank" rel="noopener noreferrer">load_query_constructor_runnable</a></td><td></td><td></td><td>Can be used to generate queries. You must specify a list of allowed operations and then return a runnable that converts a natural language query into those allowed operations.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.sql_database.query.create_sql_query_chain.html#langchain.chains.sql_database.query.create_sql_query_chain" target="_blank" rel="noopener noreferrer">create_sql_query_chain</a></td><td></td><td>SQL Database</td><td>If you want to construct a query for a SQL database from natural language.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html#langchain.chains.history_aware_retriever.create_history_aware_retriever" target="_blank" rel="noopener noreferrer">create_history_aware_retriever</a></td><td></td><td>Retriever</td><td>This chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html#langchain.chains.retrieval.create_retrieval_chain" target="_blank" rel="noopener noreferrer">create_retrieval_chain</a></td><td></td><td>Retriever</td><td>This chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response</td></tr></tbody></table>

## 2. [Legacy Chains](https://python.langchain.com/v0.1/docs/modules/chains/#legacy-chains)

Below are the legacy chains. We will maintain support for these until we create an LCEL alternative.

Table columns:

* **Chain**: Name of the chain or name of the constructor method. If constructor method, this will return a Chain subclass.
* **Function Calling**: Whether chain requires OpenAI Function Calling.
* **Other Tools**: Other tools used in the chain.
* **When to Use**: Our commentary on when to use.

<table><thead><tr><th>Chain</th><th>Function Calling</th><th>Other Tools</th><th>When to Use</th></tr></thead><tbody><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.api.base.APIChain.html#langchain.chains.api.base.APIChain" target="_blank" rel="noopener noreferrer">APIChain</a></td><td></td><td>Requests Wrapper</td><td>This chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respond</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.api.openapi.chain.OpenAPIEndpointChain.html#langchain.chains.api.openapi.chain.OpenAPIEndpointChain" target="_blank" rel="noopener noreferrer">OpenAPIEndpointChain</a></td><td></td><td>OpenAPI Spec</td><td>Similar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpoints</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain" target="_blank" rel="noopener noreferrer">ConversationalRetrievalChain</a></td><td></td><td>Retriever</td><td>This chain can be used to have <strong>conversations</strong> with a document. It takes in a question and (optional) previous conversation history. If there is a previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain" target="_blank" rel="noopener noreferrer">StuffDocumentsChain</a></td><td></td><td></td><td>This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain" target="_blank" rel="noopener noreferrer">ReduceDocumentsChain</a></td><td></td><td></td><td>This chain combines documents by iterative reducing them. It groups documents into chunks (less than some context length) and then passes them into an LLM. It then takes the responses and continues to do this until it can fit everything into one final LLM call. It is useful when you have a lot of documents, you want to have the LLM run over all of them, and you can do it in parallel.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain" target="_blank" rel="noopener noreferrer">MapReduceDocumentsChain</a></td><td></td><td></td><td>This chain first passes each document through an LLM, then reduces them using the <code>ReduceDocumentsChain</code>. It is useful in the same situations as <code>ReduceDocumentsChain</code>, but does an initial LLM call before trying to reduce the documents.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html#langchain.chains.combine_documents.refine.RefineDocumentsChain" target="_blank" rel="noopener noreferrer">RefineDocumentsChain</a></td><td></td><td></td><td>This chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to <em>refine</em> its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html#langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain" target="_blank" rel="noopener noreferrer">MapRerankDocumentsChain</a></td><td></td><td></td><td>This calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html#langchain.chains.constitutional_ai.base.ConstitutionalChain" target="_blank" rel="noopener noreferrer">ConstitutionalChain</a></td><td></td><td></td><td>This chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this to enforce that a chain's answer follows some principles.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain" target="_blank" rel="noopener noreferrer">LLMChain</a></td><td></td><td></td><td></td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.elasticsearch_database.base.ElasticsearchDatabaseChain.html#langchain.chains.elasticsearch_database.base.ElasticsearchDatabaseChain" target="_blank" rel="noopener noreferrer">ElasticsearchDatabaseChain</a></td><td></td><td>Elasticsearch Instance</td><td>This chain converts a natural language question to an <code>Elasticsearch</code> query, and then runs it, and then summarizes the response. This is useful for when you want to ask natural language questions of an <code>Elasticsearch</code> database</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.flare.base.FlareChain.html#langchain.chains.flare.base.FlareChain" target="_blank" rel="noopener noreferrer">FlareChain</a></td><td></td><td></td><td>This implements <a href="https://arxiv.org/abs/2305.06983" target="_blank" rel="noopener noreferrer">FLARE</a>, an advanced retrieval technique. It is primarily meant as an exploratory advanced retrieval method.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.arangodb.ArangoGraphQAChain.html#langchain.chains.graph_qa.arangodb.ArangoGraphQAChain" target="_blank" rel="noopener noreferrer">ArangoGraphQAChain</a></td><td></td><td>Arango Graph</td><td>This chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.cypher.GraphCypherQAChain.html#langchain.chains.graph_qa.cypher.GraphCypherQAChain" target="_blank" rel="noopener noreferrer">GraphCypherQAChain</a></td><td></td><td>A graph that works with Cypher query language</td><td>This chain constructs a Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.falkordb.FalkorDBQAChain.html#langchain.chains.graph_qa.falkordb.FalkorDBQAChain" target="_blank" rel="noopener noreferrer">FalkorDBGraphQAChain</a></td><td></td><td>Falkor Database</td><td>This chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.hugegraph.HugeGraphQAChain.html#langchain.chains.graph_qa.hugegraph.HugeGraphQAChain" target="_blank" rel="noopener noreferrer">HugeGraphQAChain</a></td><td></td><td>HugeGraph</td><td>This chain constructs an HugeGraph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.kuzu.KuzuQAChain.html#langchain.chains.graph_qa.kuzu.KuzuQAChain" target="_blank" rel="noopener noreferrer">KuzuQAChain</a></td><td></td><td>Kuzu Graph</td><td>This chain constructs a Kuzu Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain.html#langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain" target="_blank" rel="noopener noreferrer">NebulaGraphQAChain</a></td><td></td><td>Nebula Graph</td><td>This chain constructs a Nebula Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain.html#langchain.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain" target="_blank" rel="noopener noreferrer">NeptuneOpenCypherQAChain</a></td><td></td><td>Neptune Graph</td><td>This chain constructs a Neptune Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.sparql.GraphSparqlQAChain.html#langchain.chains.graph_qa.sparql.GraphSparqlQAChain" target="_blank" rel="noopener noreferrer">GraphSparqlChain</a></td><td></td><td>Graph that works with SparQL</td><td>This chain constructs a SparQL query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_math.base.LLMMathChain.html#langchain.chains.llm_math.base.LLMMathChain" target="_blank" rel="noopener noreferrer">LLMMath</a></td><td></td><td></td><td>This chain converts a user question to a math problem and then executes it (using <a href="https://github.com/pydata/numexpr" target="_blank" rel="noopener noreferrer">numexpr</a>)</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_checker.base.LLMCheckerChain.html#langchain.chains.llm_checker.base.LLMCheckerChain" target="_blank" rel="noopener noreferrer">LLMCheckerChain</a></td><td></td><td></td><td>This chain uses a second LLM call to verify its initial answer. Use this when you have an extra layer of validation on the initial LLM call.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain.html#langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain" target="_blank" rel="noopener noreferrer">LLMSummarizationChecker</a></td><td></td><td></td><td>This chain creates a summary using a sequence of LLM calls to make sure it is extra correct. Use this over the normal summarization chain when you are okay with multiple LLM calls (eg you care more about accuracy than speed/cost).</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain.html#langchain.chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain" target="_blank" rel="noopener noreferrer">create_citation_fuzzy_match_chain</a></td><td>✅</td><td></td><td>Uses OpenAI function calling to answer questions and cite its sources.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.extraction.create_extraction_chain.html#langchain.chains.openai_functions.extraction.create_extraction_chain" target="_blank" rel="noopener noreferrer">create_extraction_chain</a></td><td>✅</td><td></td><td>Uses OpenAI Function calling to extract information from text.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.extraction.create_extraction_chain_pydantic.html#langchain.chains.openai_functions.extraction.create_extraction_chain_pydantic" target="_blank" rel="noopener noreferrer">create_extraction_chain_pydantic</a></td><td>✅</td><td></td><td>Uses OpenAI function calling to extract information from text into a Pydantic model. Compared to <code>create_extraction_chain</code> this has a tighter integration with Pydantic.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.openapi.get_openapi_chain.html#langchain.chains.openai_functions.openapi.get_openapi_chain" target="_blank" rel="noopener noreferrer">get_openapi_chain</a></td><td>✅</td><td>OpenAPI Spec</td><td>Uses OpenAI function calling to query an OpenAPI.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_structure_chain.html#langchain.chains.openai_functions.qa_with_structure.create_qa_with_structure_chain" target="_blank" rel="noopener noreferrer">create_qa_with_structure_chain</a></td><td>✅</td><td></td><td>Uses OpenAI function calling to do question answering over text and respond in a specific format.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_sources_chain.html#langchain.chains.openai_functions.qa_with_structure.create_qa_with_sources_chain" target="_blank" rel="noopener noreferrer">create_qa_with_sources_chain</a></td><td>✅</td><td></td><td>Uses OpenAI function calling to answer questions with citations.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_generation.base.QAGenerationChain.html#langchain.chains.qa_generation.base.QAGenerationChain" target="_blank" rel="noopener noreferrer">QAGenerationChain</a></td><td></td><td></td><td>Creates both questions and answers from documents. Used to generate question/answer pairs for evaluation of retrieval projects.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html#langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain" target="_blank" rel="noopener noreferrer">RetrievalQAWithSourcesChain</a></td><td></td><td>Retriever</td><td>Does question answering over retrieved documents, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over <code>load_qa_with_sources_chain</code> when you want to use a retriever to fetch the relevant document as part of the chain (rather than pass them in).</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain.html#langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain" target="_blank" rel="noopener noreferrer">load_qa_with_sources_chain</a></td><td></td><td>Retriever</td><td>Does question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you want to pass in the documents directly (rather than rely on a retriever to get them).</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html#langchain.chains.retrieval_qa.base.RetrievalQA" target="_blank" rel="noopener noreferrer">RetrievalQA</a></td><td></td><td>Retriever</td><td>This chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a response.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html#langchain.chains.router.multi_prompt.MultiPromptChain" target="_blank" rel="noopener noreferrer">MultiPromptChain</a></td><td></td><td></td><td>This chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain.html#langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain" target="_blank" rel="noopener noreferrer">MultiRetrievalQAChain</a></td><td></td><td>Retriever</td><td>This chain routes input between multiple retrievers. Use this when you have multiple potential retrievers you could fetch relevant documents from and want to route to just one.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.router.embedding_router.EmbeddingRouterChain.html#langchain.chains.router.embedding_router.EmbeddingRouterChain" target="_blank" rel="noopener noreferrer">EmbeddingRouterChain</a></td><td></td><td></td><td>This chain uses embedding similarity to route incoming queries.</td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.router.llm_router.LLMRouterChain.html#langchain.chains.router.llm_router.LLMRouterChain" target="_blank" rel="noopener noreferrer">LLMRouterChain</a></td><td></td><td></td><td>This chain uses an LLM to route between potential options.</td></tr><tr><td>load_summarize_chain</td><td></td><td></td><td></td></tr><tr><td><a href="https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_requests.LLMRequestsChain.html#langchain.chains.llm_requests.LLMRequestsChain" target="_blank" rel="noopener noreferrer">LLMRequestsChain</a></td><td></td><td></td><td>This chain constructs a URL from user input, gets data at that URL, and then summarizes the response. Compared to APIChain, this chain is not focused on a single API spec but is more general</td></tr></tbody></table>